{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033772ae",
   "metadata": {},
   "source": [
    "# Zero-DCE Improved: Development Notebook\n",
    "\n",
    "**Authors:** Imrose Batterywala (314540010), Shahzeb Mohammed (314540021)\n",
    "\n",
    "This notebook implements the improvements outlined in the survey:\n",
    "1. **Bright/Dark Balance Loss** - Dual-histogram regularization\n",
    "2. **Texture-Aware Lighting Maps** - Gradient-respecting kernels\n",
    "3. **Hybrid Exposure Fusion** - Multi-exposure fusion\n",
    "4. **Perceptual Co-training** - NIMA/NIQE-based losses\n",
    "\n",
    "We'll implement and test each improvement incrementally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53dee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import baseline modules\n",
    "current_dir = Path.cwd()\n",
    "if 'improv' in str(current_dir):\n",
    "    parent_dir = current_dir.parent\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import baseline modules\n",
    "import model\n",
    "import Myloss\n",
    "import dataloader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set paths - adjust based on notebook location\n",
    "PROJECT_ROOT = parent_dir\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa279b",
   "metadata": {},
   "source": [
    "## 1. Bright/Dark Balance Loss\n",
    "\n",
    "This loss addresses the dual problem of persistent dark pixels and missing bright pixels by:\n",
    "- Penalizing high fraction of pixels < 0.2 (dark regions)\n",
    "- Encouraging presence of pixels > 0.9 (bright regions)\n",
    "- Balancing with existing exposure control loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrightDarkBalanceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-histogram regularization loss to address:\n",
    "    - Persistent dark pixels (< 0.2)\n",
    "    - Missing bright pixels (> 0.9)\n",
    "    \"\"\"\n",
    "    def __init__(self, dark_threshold=0.2, bright_threshold=0.9, \n",
    "                 dark_target=0.05, bright_target=0.01):\n",
    "        super(BrightDarkBalanceLoss, self).__init__()\n",
    "        self.dark_threshold = dark_threshold\n",
    "        self.bright_threshold = bright_threshold\n",
    "        self.dark_target = dark_target  # Target fraction of dark pixels\n",
    "        self.bright_target = bright_target  # Target fraction of bright pixels\n",
    "        \n",
    "    def compute_luminance(self, x):\n",
    "        \"\"\"Convert RGB to luminance\"\"\"\n",
    "        # Using standard luminance weights: 0.299*R + 0.587*G + 0.114*B\n",
    "        if x.shape[1] == 3:  # RGB image\n",
    "            weights = torch.tensor([0.299, 0.587, 0.114], device=x.device).view(1, 3, 1, 1)\n",
    "            luma = torch.sum(x * weights, dim=1, keepdim=True)\n",
    "        else:\n",
    "            luma = x\n",
    "        return luma\n",
    "    \n",
    "    def forward(self, enhanced_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            enhanced_image: Tensor of shape (B, C, H, W) in range [0, 1]\n",
    "        Returns:\n",
    "            loss: Combined dark and bright pixel balance loss\n",
    "        \"\"\"\n",
    "        # Compute luminance\n",
    "        luma = self.compute_luminance(enhanced_image)\n",
    "        \n",
    "        # Compute dark pixel fraction (pixels < dark_threshold)\n",
    "        dark_mask = (luma < self.dark_threshold).float()\n",
    "        dark_fraction = torch.mean(dark_mask)\n",
    "        \n",
    "        # Compute bright pixel fraction (pixels > bright_threshold)\n",
    "        bright_mask = (luma > self.bright_threshold).float()\n",
    "        bright_fraction = torch.mean(bright_mask)\n",
    "        \n",
    "        # Dark pixel reduction loss: penalize high dark fraction\n",
    "        # Use L2 loss to push dark_fraction toward target\n",
    "        dark_loss = torch.pow(dark_fraction - self.dark_target, 2)\n",
    "        \n",
    "        # Bright pixel promotion loss: encourage presence of bright pixels\n",
    "        # Use inverse relationship - if bright_fraction is too low, increase loss\n",
    "        if bright_fraction < self.bright_target:\n",
    "            bright_loss = torch.pow(self.bright_target - bright_fraction, 2)\n",
    "        else:\n",
    "            # If we have enough bright pixels, just maintain (small penalty)\n",
    "            bright_loss = 0.1 * torch.pow(bright_fraction - self.bright_target, 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = dark_loss + bright_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'dark_fraction': dark_fraction.item(),\n",
    "            'bright_fraction': bright_fraction.item(),\n",
    "            'dark_loss': dark_loss.item(),\n",
    "            'bright_loss': bright_loss.item()\n",
    "        }\n",
    "\n",
    "# Test the loss function\n",
    "print(\"Bright/Dark Balance Loss implementation complete!\")\n",
    "print(\"\\nLoss function parameters:\")\n",
    "print(f\"  Dark threshold: 0.2\")\n",
    "print(f\"  Bright threshold: 0.9\")\n",
    "print(f\"  Dark target fraction: 0.05\")\n",
    "print(f\"  Bright target fraction: 0.01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bb4a9",
   "metadata": {},
   "source": [
    "## 2. Texture-Aware Lighting Maps\n",
    "\n",
    "This improvement uses gradient-respecting kernels to preserve texture while enhancing illumination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureAwareSmoothnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Gradient-aware illumination smoothness loss that respects texture boundaries.\n",
    "    This prevents harsh contrast jumps while maintaining smooth illumination maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TextureAwareSmoothnessLoss, self).__init__()\n",
    "        \n",
    "        # Sobel kernels for gradient computation\n",
    "        sobel_x = torch.FloatTensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.FloatTensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3)\n",
    "        \n",
    "        self.register_buffer('sobel_x', sobel_x)\n",
    "        self.register_buffer('sobel_y', sobel_y)\n",
    "        \n",
    "    def compute_gradient_magnitude(self, x):\n",
    "        \"\"\"Compute gradient magnitude map\"\"\"\n",
    "        # Convert to grayscale if RGB\n",
    "        if x.shape[1] == 3:\n",
    "            weights = torch.tensor([0.299, 0.587, 0.114], device=x.device).view(1, 3, 1, 1)\n",
    "            gray = torch.sum(x * weights, dim=1, keepdim=True)\n",
    "        else:\n",
    "            gray = x\n",
    "            \n",
    "        # Compute gradients\n",
    "        grad_x = F.conv2d(gray, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(gray, self.sobel_y, padding=1)\n",
    "        \n",
    "        # Gradient magnitude\n",
    "        grad_mag = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
    "        return grad_mag\n",
    "    \n",
    "    def forward(self, illumination_map, input_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            illumination_map: Curve parameter maps (B, 24, H, W) or similar\n",
    "            input_image: Original input image (B, 3, H, W) for gradient computation\n",
    "        Returns:\n",
    "            loss: Gradient-aware smoothness loss\n",
    "        \"\"\"\n",
    "        # Compute gradient magnitude of input image\n",
    "        input_grad = self.compute_gradient_magnitude(input_image)\n",
    "        \n",
    "        # Normalize gradient to [0, 1] for weighting\n",
    "        input_grad_norm = (input_grad - input_grad.min()) / (input_grad.max() - input_grad.min() + 1e-6)\n",
    "        \n",
    "        # Compute gradients of illumination map\n",
    "        # For each channel in illumination map\n",
    "        total_loss = 0.0\n",
    "        num_channels = illumination_map.shape[1]\n",
    "        \n",
    "        for i in range(num_channels):\n",
    "            channel_map = illumination_map[:, i:i+1, :, :]\n",
    "            \n",
    "            # Compute horizontal and vertical gradients\n",
    "            grad_x = torch.abs(channel_map[:, :, :, 1:] - channel_map[:, :, :, :-1])\n",
    "            grad_y = torch.abs(channel_map[:, :, 1:, :] - channel_map[:, :, :-1, :])\n",
    "            \n",
    "            # Apply gradient-aware weighting\n",
    "            # In high-gradient regions (texture boundaries), allow more variation\n",
    "            # In low-gradient regions (smooth areas), enforce more smoothness\n",
    "            weight_x = 1.0 / (input_grad_norm[:, :, :, :-1] + 0.1)  # Inverse weighting\n",
    "            weight_y = 1.0 / (input_grad_norm[:, :, :-1, :] + 0.1)\n",
    "            \n",
    "            # Weighted smoothness loss\n",
    "            channel_loss = torch.mean(weight_x * grad_x) + torch.mean(weight_y * grad_y)\n",
    "            total_loss += channel_loss\n",
    "        \n",
    "        return total_loss / num_channels\n",
    "\n",
    "print(\"Texture-Aware Smoothness Loss implementation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae2580",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExposureFusion:\n",
    "    \"\"\"\n",
    "    Multi-scale exposure fusion to combine Zero-DCE output with synthetic exposure brackets.\n",
    "    This helps preserve highlights while lifting shadows.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales=5, exposure_values=[-2, -1, 0, 1, 2]):\n",
    "        self.num_scales = num_scales\n",
    "        self.exposure_values = exposure_values  # Exposure values in EV\n",
    "        \n",
    "    def generate_exposure_brackets(self, image, base_exposure=0):\n",
    "        \"\"\"\n",
    "        Generate synthetic exposure brackets from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Tensor (B, C, H, W) in range [0, 1]\n",
    "            base_exposure: Base exposure value (0 = no change)\n",
    "        Returns:\n",
    "            brackets: List of exposure-bracketed images\n",
    "        \"\"\"\n",
    "        brackets = []\n",
    "        for ev in self.exposure_values:\n",
    "            # Exposure adjustment: multiply by 2^EV\n",
    "            exposure_factor = 2.0 ** ev\n",
    "            exposed = image * exposure_factor\n",
    "            # Clip to valid range\n",
    "            exposed = torch.clamp(exposed, 0.0, 1.0)\n",
    "            brackets.append(exposed)\n",
    "        return brackets\n",
    "    \n",
    "    def compute_weights(self, image):\n",
    "        \"\"\"\n",
    "        Compute fusion weights based on:\n",
    "        - Well-exposedness (closeness to 0.5)\n",
    "        - Saturation (colorfulness)\n",
    "        - Contrast (local variance)\n",
    "        \"\"\"\n",
    "        B, C, H, W = image.shape\n",
    "        \n",
    "        # Well-exposedness: Gaussian centered at 0.5\n",
    "        well_exposed = torch.exp(-0.5 * torch.pow(image - 0.5, 2) / (0.2 ** 2))\n",
    "        well_exposed = torch.mean(well_exposed, dim=1, keepdim=True)  # Average across channels\n",
    "        \n",
    "        # Saturation: standard deviation across color channels\n",
    "        saturation = torch.std(image, dim=1, keepdim=True)\n",
    "        \n",
    "        # Contrast: local variance using a simple kernel\n",
    "        kernel_size = 5\n",
    "        kernel = torch.ones(1, 1, kernel_size, kernel_size, device=image.device) / (kernel_size ** 2)\n",
    "        mean = F.conv2d(torch.mean(image, dim=1, keepdim=True), kernel, padding=kernel_size//2)\n",
    "        variance = F.conv2d(torch.pow(torch.mean(image, dim=1, keepdim=True) - mean, 2), \n",
    "                          kernel, padding=kernel_size//2)\n",
    "        contrast = variance\n",
    "        \n",
    "        # Normalize each component\n",
    "        well_exposed = (well_exposed - well_exposed.min()) / (well_exposed.max() - well_exposed.min() + 1e-6)\n",
    "        saturation = (saturation - saturation.min()) / (saturation.max() - saturation.min() + 1e-6)\n",
    "        contrast = (contrast - contrast.min()) / (contrast.max() - contrast.min() + 1e-6)\n",
    "        \n",
    "        # Combined weight\n",
    "        weight = well_exposed * saturation * (contrast + 0.1)\n",
    "        return weight\n",
    "    \n",
    "    def multi_scale_fusion(self, images, weights):\n",
    "        \"\"\"\n",
    "        Multi-scale fusion using Laplacian pyramid.\n",
    "        \n",
    "        Args:\n",
    "            images: List of images to fuse\n",
    "            weights: List of weight maps\n",
    "        Returns:\n",
    "            fused: Fused image\n",
    "        \"\"\"\n",
    "        # Simple weighted average (can be extended to Laplacian pyramid)\n",
    "        weights_sum = sum(weights)\n",
    "        weights_sum = torch.clamp(weights_sum, min=1e-6)\n",
    "        \n",
    "        fused = sum(img * w for img, w in zip(images, weights)) / weights_sum\n",
    "        return fused\n",
    "    \n",
    "    def fuse(self, zero_dce_output, input_image):\n",
    "        \"\"\"\n",
    "        Fuse Zero-DCE output with exposure brackets.\n",
    "        \n",
    "        Args:\n",
    "            zero_dce_output: Enhanced image from Zero-DCE (B, C, H, W)\n",
    "            input_image: Original input image (B, C, H, W)\n",
    "        Returns:\n",
    "            fused_image: Fused result\n",
    "        \"\"\"\n",
    "        # Generate exposure brackets from Zero-DCE output\n",
    "        brackets = self.generate_exposure_brackets(zero_dce_output)\n",
    "        \n",
    "        # Include Zero-DCE output as one of the brackets\n",
    "        all_images = [zero_dce_output] + brackets\n",
    "        \n",
    "        # Compute weights for each image\n",
    "        weights = [self.compute_weights(img) for img in all_images]\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        fused = self.multi_scale_fusion(all_images, weights)\n",
    "        \n",
    "        # Adaptive blending with Zero-DCE output based on local luminance\n",
    "        luma_weights = torch.tensor([0.299, 0.587, 0.114], device=zero_dce_output.device).view(1, 3, 1, 1)\n",
    "        luma = torch.sum(zero_dce_output * luma_weights, dim=1, keepdim=True)\n",
    "        \n",
    "        # In dark regions, use more of fused result; in bright regions, use more of Zero-DCE\n",
    "        blend_weight = torch.clamp(luma * 2.0, 0.0, 1.0)  # More fusion in dark areas\n",
    "        final = blend_weight * fused + (1 - blend_weight) * zero_dce_output\n",
    "        \n",
    "        return torch.clamp(final, 0.0, 1.0)\n",
    "\n",
    "print(\"Exposure Fusion implementation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ad810",
   "metadata": {},
   "source": [
    "## 4. Perceptual Co-training Loss\n",
    "\n",
    "This loss uses NIMA and NIQE metrics to optimize for perceptual quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28637bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG features (similar to existing perception_loss in Myloss.py)\n",
    "    Extended to work with NIMA/NIQE concepts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        # Use VGG features for perceptual similarity\n",
    "        # This is a simplified version - full NIMA/NIQE integration would require their models\n",
    "        try:\n",
    "            from torchvision.models import vgg16\n",
    "            vgg = vgg16(pretrained=True).features\n",
    "            self.feature_extractor = nn.Sequential(*list(vgg.children())[:23])  # Up to relu4_3\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        except:\n",
    "            print(\"Warning: Could not load VGG16. Using simplified perceptual loss.\")\n",
    "            self.feature_extractor = None\n",
    "    \n",
    "    def forward(self, enhanced_image, target_image=None):\n",
    "        \"\"\"\n",
    "        Compute perceptual loss.\n",
    "        If target_image is None, assumes we want to maximize quality (no reference).\n",
    "        \"\"\"\n",
    "        if self.feature_extractor is None:\n",
    "            # Fallback: use simple L2 loss on image\n",
    "            return torch.mean(torch.pow(enhanced_image, 2))\n",
    "        \n",
    "        # Extract features\n",
    "        enhanced_features = self.feature_extractor(enhanced_image)\n",
    "        \n",
    "        if target_image is not None:\n",
    "            # Reference-based perceptual loss\n",
    "            target_features = self.feature_extractor(target_image)\n",
    "            loss = F.mse_loss(enhanced_features, target_features)\n",
    "        else:\n",
    "            # No-reference: encourage natural-looking features\n",
    "            # Penalize extreme values (overexposure/underexposure)\n",
    "            loss = torch.mean(torch.pow(enhanced_features - 0.5, 2))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Note: Full NIMA/NIQE integration would require:\n",
    "# 1. Loading pre-trained NIMA model for aesthetic scoring\n",
    "# 2. Computing NIQE score (typically non-differentiable, may need approximation)\n",
    "# For now, we use VGG-based perceptual loss as a proxy\n",
    "\n",
    "print(\"Perceptual Loss implementation complete!\")\n",
    "print(\"Note: Full NIMA/NIQE integration requires their pre-trained models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc088a",
   "metadata": {},
   "source": [
    "## 5. Integrated Training Function\n",
    "\n",
    "Combine all improvements into a unified training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f15f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_loss_function(use_bright_dark=True, use_texture_aware=True, \n",
    "                                   use_perceptual=True, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a combined loss function with all improvements.\n",
    "    \n",
    "    Returns:\n",
    "        loss_fn: Function that computes total loss\n",
    "        loss_components: Dictionary to store individual loss values\n",
    "    \"\"\"\n",
    "    # Baseline losses\n",
    "    L_color = Myloss.L_color().to(device)\n",
    "    L_spa = Myloss.L_spa().to(device)\n",
    "    L_exp = Myloss.L_exp(16, 0.6).to(device)\n",
    "    L_TV = Myloss.L_TV().to(device)\n",
    "    \n",
    "    # New losses\n",
    "    bright_dark_loss = BrightDarkBalanceLoss().to(device) if use_bright_dark else None\n",
    "    texture_aware_loss = TextureAwareSmoothnessLoss().to(device) if use_texture_aware else None\n",
    "    perceptual_loss = PerceptualLoss().to(device) if use_perceptual else None\n",
    "    \n",
    "    def compute_loss(enhanced_image_1, enhanced_image, A, img_lowlight, \n",
    "                     loss_weights=None):\n",
    "        \"\"\"\n",
    "        Compute combined loss with all improvements.\n",
    "        \n",
    "        Args:\n",
    "            enhanced_image_1: Intermediate enhanced image\n",
    "            enhanced_image: Final enhanced image\n",
    "            A: Illumination adjustment curves (24 channels)\n",
    "            img_lowlight: Original low-light input\n",
    "            loss_weights: Dictionary of loss weights (optional)\n",
    "        Returns:\n",
    "            total_loss: Combined loss\n",
    "            loss_dict: Dictionary of individual losses\n",
    "        \"\"\"\n",
    "        if loss_weights is None:\n",
    "            loss_weights = {\n",
    "                'tv': 200.0,\n",
    "                'spa': 1.0,\n",
    "                'color': 5.0,\n",
    "                'exp': 10.0,\n",
    "                'bright_dark': 2.0,\n",
    "                'texture_aware': 1.0,\n",
    "                'perceptual': 0.5\n",
    "            }\n",
    "        \n",
    "        loss_dict = {}\n",
    "        \n",
    "        # Baseline losses\n",
    "        loss_tv = loss_weights['tv'] * L_TV(A)\n",
    "        loss_spa = loss_weights['spa'] * torch.mean(L_spa(enhanced_image, img_lowlight))\n",
    "        loss_col = loss_weights['color'] * torch.mean(L_color(enhanced_image))\n",
    "        loss_exp = loss_weights['exp'] * torch.mean(L_exp(enhanced_image))\n",
    "        \n",
    "        loss_dict['tv'] = loss_tv.item()\n",
    "        loss_dict['spa'] = loss_spa.item()\n",
    "        loss_dict['color'] = loss_col.item()\n",
    "        loss_dict['exp'] = loss_exp.item()\n",
    "        \n",
    "        total_loss = loss_tv + loss_spa + loss_col + loss_exp\n",
    "        \n",
    "        # New losses\n",
    "        if bright_dark_loss is not None:\n",
    "            bd_loss, bd_info = bright_dark_loss(enhanced_image)\n",
    "            total_loss += loss_weights['bright_dark'] * bd_loss\n",
    "            loss_dict['bright_dark'] = bd_loss.item()\n",
    "            loss_dict.update({f'bd_{k}': v for k, v in bd_info.items()})\n",
    "        \n",
    "        if texture_aware_loss is not None:\n",
    "            ta_loss = texture_aware_loss(A, img_lowlight)\n",
    "            total_loss += loss_weights['texture_aware'] * ta_loss\n",
    "            loss_dict['texture_aware'] = ta_loss.item()\n",
    "        \n",
    "        if perceptual_loss is not None:\n",
    "            perc_loss = perceptual_loss(enhanced_image)\n",
    "            total_loss += loss_weights['perceptual'] * perc_loss\n",
    "            loss_dict['perceptual'] = perc_loss.item()\n",
    "        \n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "    \n",
    "    return compute_loss\n",
    "\n",
    "print(\"Integrated loss function created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049c798",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluation\n",
    "\n",
    "Let's test the improvements on a sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e445253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "def load_model(weights_path, device):\n",
    "    \"\"\"Load Zero-DCE model\"\"\"\n",
    "    net = model.enhance_net_nopool().to(device)\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "# Test on a sample image\n",
    "def test_improvements(image_path, model_path, device='cuda', use_fusion=True):\n",
    "    \"\"\"\n",
    "    Test all improvements on a single image.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    net = load_model(model_path, device)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.asarray(img).astype(np.float32) / 255.0\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Baseline enhancement\n",
    "    with torch.no_grad():\n",
    "        enhanced_1, enhanced, curves = net(img_tensor)\n",
    "    \n",
    "    # Apply exposure fusion if enabled\n",
    "    if use_fusion:\n",
    "        fusion = ExposureFusion()\n",
    "        enhanced_fused = fusion.fuse(enhanced, img_tensor)\n",
    "    else:\n",
    "        enhanced_fused = enhanced\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    def tensor_to_numpy(t):\n",
    "        t = t.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "        return np.clip(t, 0, 1)\n",
    "    \n",
    "    original = tensor_to_numpy(img_tensor)\n",
    "    enhanced_np = tensor_to_numpy(enhanced)\n",
    "    enhanced_fused_np = tensor_to_numpy(enhanced_fused) if use_fusion else None\n",
    "    \n",
    "    return {\n",
    "        'original': original,\n",
    "        'enhanced': enhanced_np,\n",
    "        'enhanced_fused': enhanced_fused_np,\n",
    "        'curves': curves\n",
    "    }\n",
    "\n",
    "print(\"Testing functions ready!\")\n",
    "print(\"\\nTo test, run:\")\n",
    "print(\"results = test_improvements('path/to/image.jpg', 'snapshots/Epoch99.pth', device)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e6640",
   "metadata": {},
   "source": [
    "## 7. Visualization Helper\n",
    "\n",
    "Function to visualize results and compare baseline vs improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize original, enhanced, and fused results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3 if results['enhanced_fused'] is not None else 2, \n",
    "                            figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(results['original'])\n",
    "    axes[0].set_title('Original Low-Light Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(results['enhanced'])\n",
    "    axes[1].set_title('Zero-DCE Enhanced')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    if results['enhanced_fused'] is not None:\n",
    "        axes[2].imshow(results['enhanced_fused'])\n",
    "        axes[2].set_title('With Exposure Fusion')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def compute_metrics(image):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for an image.\n",
    "    \"\"\"\n",
    "    # Convert to luminance\n",
    "    if len(image.shape) == 3:\n",
    "        luma = 0.299 * image[:, :, 0] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 2]\n",
    "    else:\n",
    "        luma = image\n",
    "    \n",
    "    # Dark pixel fraction\n",
    "    dark_fraction = np.mean(luma < 0.2)\n",
    "    \n",
    "    # Bright pixel fraction\n",
    "    bright_fraction = np.mean(luma > 0.9)\n",
    "    \n",
    "    # Patch contrast (4x4 grid)\n",
    "    h, w = luma.shape\n",
    "    patch_h, patch_w = h // 4, w // 4\n",
    "    patches = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            patch = luma[i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w]\n",
    "            patches.append(np.std(patch))\n",
    "    patch_contrast = np.mean(patches)\n",
    "    \n",
    "    return {\n",
    "        'dark_fraction': dark_fraction,\n",
    "        'bright_fraction': bright_fraction,\n",
    "        'patch_contrast': patch_contrast,\n",
    "        'mean_luminance': np.mean(luma)\n",
    "    }\n",
    "\n",
    "print(\"Visualization and metrics functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d3cc6",
   "metadata": {},
   "source": [
    "## 8. Example Usage\n",
    "\n",
    "Test the improvements on a sample image from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57346410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test on a sample image\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# # Set paths\n",
    "# test_image_path = PROJECT_ROOT / \"data/test_data/DICM/01.jpg\"\n",
    "# model_path = PROJECT_ROOT / \"snapshots/Epoch99.pth\"\n",
    "# \n",
    "# if test_image_path.exists() and model_path.exists():\n",
    "#     print(\"Testing improvements on sample image...\")\n",
    "#     results = test_improvements(str(test_image_path), str(model_path), device)\n",
    "#     \n",
    "#     # Compute metrics\n",
    "#     orig_metrics = compute_metrics(results['original'])\n",
    "#     enh_metrics = compute_metrics(results['enhanced'])\n",
    "#     \n",
    "#     print(\"\\n=== Metrics Comparison ===\")\n",
    "#     print(f\"Dark Pixel Fraction: {orig_metrics['dark_fraction']:.4f} -> {enh_metrics['dark_fraction']:.4f}\")\n",
    "#     print(f\"Bright Pixel Fraction: {orig_metrics['bright_fraction']:.6f} -> {enh_metrics['bright_fraction']:.6f}\")\n",
    "#     print(f\"Patch Contrast: {orig_metrics['patch_contrast']:.4f} -> {enh_metrics['patch_contrast']:.4f}\")\n",
    "#     \n",
    "#     # Visualize\n",
    "#     visualize_results(results)\n",
    "# else:\n",
    "#     print(\"Test image or model not found. Please check paths.\")\n",
    "\n",
    "print(\"Ready to test! Uncomment the code above and adjust paths as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fd7b2",
   "metadata": {},
   "source": [
    "## 9. Training Script Integration\n",
    "\n",
    "To use these improvements in training, integrate them into the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop with improvements\n",
    "# This can be integrated into lowlight_train.py\n",
    "\n",
    "def train_with_improvements(config, use_bright_dark=True, use_texture_aware=True, \n",
    "                            use_perceptual=True, use_fusion=False):\n",
    "    \"\"\"\n",
    "    Training function with all improvements integrated.\n",
    "    Note: use_fusion=False during training (only for inference)\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    DCE_net = model.enhance_net_nopool().to(device)\n",
    "    DCE_net.apply(lambda m: m.weight.data.normal_(0.0, 0.02) if isinstance(m, nn.Conv2d) else None)\n",
    "    \n",
    "    if config.load_pretrain:\n",
    "        DCE_net.load_state_dict(torch.load(config.pretrain_dir, map_location=device))\n",
    "    \n",
    "    # Data loader\n",
    "    train_dataset = dataloader.lowlight_loader(config.lowlight_images_path)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.train_batch_size, \n",
    "        shuffle=True, num_workers=config.num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Loss function with improvements\n",
    "    compute_loss = create_improved_loss_function(\n",
    "        use_bright_dark=use_bright_dark,\n",
    "        use_texture_aware=use_texture_aware,\n",
    "        use_perceptual=use_perceptual,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(DCE_net.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    DCE_net.train()\n",
    "    \n",
    "    print(\"Starting training with improvements...\")\n",
    "    print(f\"  Bright/Dark Balance: {use_bright_dark}\")\n",
    "    print(f\"  Texture-Aware: {use_texture_aware}\")\n",
    "    print(f\"  Perceptual: {use_perceptual}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        for iteration, img_lowlight in enumerate(train_loader):\n",
    "            img_lowlight = img_lowlight.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            enhanced_image_1, enhanced_image, A = DCE_net(img_lowlight)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_dict = compute_loss(enhanced_image_1, enhanced_image, A, img_lowlight)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(DCE_net.parameters(), config.grad_clip_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logging\n",
    "            if (iteration + 1) % config.display_iter == 0:\n",
    "                print(f\"Epoch {epoch}, Iteration {iteration+1}\")\n",
    "                print(f\"  Total Loss: {loss_dict['total']:.4f}\")\n",
    "                if 'bright_dark' in loss_dict:\n",
    "                    print(f\"  Dark Fraction: {loss_dict.get('bd_dark_fraction', 0):.4f}, \"\n",
    "                          f\"Bright Fraction: {loss_dict.get('bd_bright_fraction', 0):.6f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (iteration + 1) % config.snapshot_iter == 0:\n",
    "                save_path = config.snapshots_folder + f\"Epoch{epoch}_improved.pth\"\n",
    "                torch.save(DCE_net.state_dict(), save_path)\n",
    "                print(f\"  Saved checkpoint: {save_path}\")\n",
    "    \n",
    "    return DCE_net\n",
    "\n",
    "print(\"Training function with improvements ready!\")\n",
    "print(\"\\nTo use, create a config object and call:\")\n",
    "print(\"  model = train_with_improvements(config)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4974a9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements all four improvements from the survey:\n",
    "\n",
    "1. ✅ **Bright/Dark Balance Loss** - Dual-histogram regularization\n",
    "2. ✅ **Texture-Aware Lighting Maps** - Gradient-respecting smoothness\n",
    "3. ✅ **Hybrid Exposure Fusion** - Multi-exposure fusion (for inference)\n",
    "4. ✅ **Perceptual Co-training** - VGG-based perceptual loss\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Test individual components** on sample images\n",
    "2. **Fine-tune loss weights** for optimal balance\n",
    "3. **Train model** with improvements (start with one improvement at a time)\n",
    "4. **Evaluate** on test datasets and compare with baseline\n",
    "5. **Iterate** based on results\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- Start by testing with `use_bright_dark=True` only, then add others incrementally\n",
    "- Adjust loss weights in `create_improved_loss_function()` based on results\n",
    "- Use exposure fusion only during inference (not training) to save computation\n",
    "- Monitor dark/bright pixel fractions during training to track improvement\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
