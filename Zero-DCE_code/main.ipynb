{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5ba3a3",
   "metadata": {},
   "source": [
    "# Zero-DCE Improved: Development Notebook\n",
    "\n",
    "**Authors:** Imrose Batterywala (314540010), Shahzeb Mohammed (314540021)\n",
    "\n",
    "This notebook implements the improvements outlined in the survey:\n",
    "1. **Bright/Dark Balance Loss** - Dual-histogram regularization\n",
    "2. **Texture-Aware Lighting Maps** - Gradient-respecting kernels\n",
    "3. **Hybrid Exposure Fusion** - Multi-exposure fusion\n",
    "4. **Perceptual Co-training** - NIMA/NIQE-based losses\n",
    "\n",
    "We'll implement and test each improvement incrementally.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Setup: Install Dependencies\n",
    "\n",
    "**Run the cell below first** to install required packages if they're not already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762cce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ torch is already installed\n",
      "✓ torchvision is already installed\n",
      "✓ numpy is already installed\n",
      "✓ PIL is already installed\n",
      "✓ matplotlib is already installed\n",
      "✓ cv2 is already installed\n",
      "✓ pyiqa is already installed\n",
      "\n",
      "✓ All required packages are installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "# Uncomment and run this cell if packages are not installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"✗ Failed to install {package}\")\n",
    "\n",
    "# Check if packages are installed\n",
    "required_packages = {\n",
    "    'torch': 'torch',\n",
    "    'torchvision': 'torchvision', \n",
    "    'numpy': 'numpy',\n",
    "    'PIL': 'Pillow',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'cv2': 'opencv-python',\n",
    "    'pyiqa': 'pyiqa'  # For NIMA/NIQE metrics (optional)\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for module_name, package_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"✓ {module_name} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"✗ {module_name} is missing\")\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nInstalling missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    for package in missing_packages:\n",
    "        install_package(package)\n",
    "    \n",
    "    print(\"\\n✓ Installation complete! Please restart the kernel and run the next cell.\")\n",
    "    print(\"To restart: Kernel -> Restart Kernel\")\n",
    "else:\n",
    "    print(\"\\n✓ All required packages are installed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033772ae",
   "metadata": {},
   "source": [
    "# Zero-DCE Improved: Development Notebook\n",
    "\n",
    "**Authors:** Imrose Batterywala (314540010), Shahzeb Mohammed (314540021)\n",
    "\n",
    "This notebook implements the improvements outlined in the survey:\n",
    "1. **Bright/Dark Balance Loss** - Dual-histogram regularization\n",
    "2. **Texture-Aware Lighting Maps** - Gradient-respecting kernels\n",
    "3. **Hybrid Exposure Fusion** - Multi-exposure fusion\n",
    "4. **Perceptual Co-training** - NIMA/NIQE-based losses\n",
    "\n",
    "We'll implement and test each improvement incrementally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53dee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code\n",
      "Project root: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code\n",
      "Looking for model.py at: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/model.py\n",
      "✓ Found model.py\n",
      "✓ PyTorch version: 2.8.0\n",
      "✓ Successfully imported baseline modules (model, Myloss, dataloader)\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if torch is installed first\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"⚠ PyTorch is not installed!\")\n",
    "    print(\"Please run the installation cell above (Cell 1) first.\")\n",
    "    print(\"Or install manually: !pip install torch torchvision\")\n",
    "    raise ImportError(\"PyTorch is required. Please install it first.\")\n",
    "\n",
    "# Method 1: Try to find the Zero-DCE_code directory by looking for model.py\n",
    "# This works regardless of where Jupyter was launched from\n",
    "def find_project_root():\n",
    "    \"\"\"Find the directory containing model.py\"\"\"\n",
    "    # Start from current working directory\n",
    "    current = Path(os.getcwd()).resolve()\n",
    "    \n",
    "    # Check current directory and all parents\n",
    "    for path in [current] + list(current.parents):\n",
    "        if (path / 'model.py').exists():\n",
    "            return path\n",
    "    \n",
    "    # If not found, try relative to this notebook's location\n",
    "    # In Jupyter, we can use __file__ if available, but it's not always available\n",
    "    # So we'll try a few common locations\n",
    "    possible_paths = [\n",
    "        Path('/Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code'),\n",
    "        current.parent if current.name == 'improv' else current,\n",
    "        current / '..' / '..' if 'improv' in str(current) else current,\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        path = Path(path).resolve()\n",
    "        if (path / 'model.py').exists():\n",
    "            return path\n",
    "    \n",
    "    # Last resort: return parent of current if we're in improv\n",
    "    if current.name == 'improv' or 'improv' in str(current):\n",
    "        return current.parent\n",
    "    \n",
    "    return current\n",
    "\n",
    "# Find and add project root to path\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Looking for model.py at: {PROJECT_ROOT / 'model.py'}\")\n",
    "\n",
    "# Verify model.py exists\n",
    "if (PROJECT_ROOT / 'model.py').exists():\n",
    "    print(f\"✓ Found model.py\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: model.py not found!\")\n",
    "    print(f\"Please manually set PROJECT_ROOT to the directory containing model.py\")\n",
    "    print(f\"For example: PROJECT_ROOT = Path('/path/to/Zero-DCE_code')\")\n",
    "\n",
    "# Import PyTorch modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Import other dependencies\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import baseline modules\n",
    "try:\n",
    "    import model\n",
    "    import Myloss\n",
    "    import dataloader\n",
    "    print(\"✓ Successfully imported baseline modules (model, Myloss, dataloader)\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"1. Current sys.path entries: {[p for p in sys.path if p]}\")\n",
    "    print(f\"2. Try manually adding the path:\")\n",
    "    print(f\"   import sys\")\n",
    "    print(f\"   sys.path.insert(0, '/Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code')\")\n",
    "    print(f\"3. Or change to the correct directory:\")\n",
    "    print(f\"   %cd /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code\")\n",
    "    raise\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa279b",
   "metadata": {},
   "source": [
    "## 1. Bright/Dark Balance Loss\n",
    "\n",
    "This loss addresses the dual problem of persistent dark pixels and missing bright pixels by:\n",
    "- Penalizing high fraction of pixels < 0.2 (dark regions)\n",
    "- Encouraging presence of pixels > 0.9 (bright regions)\n",
    "- Balancing with existing exposure control loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "538cf5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bright/Dark Balance Loss implementation complete!\n",
      "\n",
      "Loss function parameters:\n",
      "  Dark threshold: 0.2\n",
      "  Bright threshold: 0.9\n",
      "  Dark target fraction: 0.05\n",
      "  Bright target fraction: 0.01\n"
     ]
    }
   ],
   "source": [
    "class BrightDarkBalanceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-histogram regularization loss to address:\n",
    "    - Persistent dark pixels (< 0.2)\n",
    "    - Missing bright pixels (> 0.9)\n",
    "    \"\"\"\n",
    "    def __init__(self, dark_threshold=0.2, bright_threshold=0.9, \n",
    "                 dark_target=0.05, bright_target=0.01):\n",
    "        super(BrightDarkBalanceLoss, self).__init__()\n",
    "        self.dark_threshold = dark_threshold\n",
    "        self.bright_threshold = bright_threshold\n",
    "        self.dark_target = dark_target  # Target fraction of dark pixels\n",
    "        self.bright_target = bright_target  # Target fraction of bright pixels\n",
    "        \n",
    "    def compute_luminance(self, x):\n",
    "        \"\"\"Convert RGB to luminance\"\"\"\n",
    "        # Using standard luminance weights: 0.299*R + 0.587*G + 0.114*B\n",
    "        if x.shape[1] == 3:  # RGB image\n",
    "            weights = torch.tensor([0.299, 0.587, 0.114], device=x.device).view(1, 3, 1, 1)\n",
    "            luma = torch.sum(x * weights, dim=1, keepdim=True)\n",
    "        else:\n",
    "            luma = x\n",
    "        return luma\n",
    "    \n",
    "    def forward(self, enhanced_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            enhanced_image: Tensor of shape (B, C, H, W) in range [0, 1]\n",
    "        Returns:\n",
    "            loss: Combined dark and bright pixel balance loss\n",
    "        \"\"\"\n",
    "        # Compute luminance\n",
    "        luma = self.compute_luminance(enhanced_image)\n",
    "        \n",
    "        # Compute dark pixel fraction (pixels < dark_threshold)\n",
    "        dark_mask = (luma < self.dark_threshold).float()\n",
    "        dark_fraction = torch.mean(dark_mask)\n",
    "        \n",
    "        # Compute bright pixel fraction (pixels > bright_threshold)\n",
    "        bright_mask = (luma > self.bright_threshold).float()\n",
    "        bright_fraction = torch.mean(bright_mask)\n",
    "        \n",
    "        # Dark pixel reduction loss: penalize high dark fraction\n",
    "        # Use L2 loss to push dark_fraction toward target\n",
    "        dark_loss = torch.pow(dark_fraction - self.dark_target, 2)\n",
    "        \n",
    "        # Bright pixel promotion loss: encourage presence of bright pixels\n",
    "        # Use inverse relationship - if bright_fraction is too low, increase loss\n",
    "        if bright_fraction < self.bright_target:\n",
    "            bright_loss = torch.pow(self.bright_target - bright_fraction, 2)\n",
    "        else:\n",
    "            # If we have enough bright pixels, just maintain (small penalty)\n",
    "            bright_loss = 0.1 * torch.pow(bright_fraction - self.bright_target, 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = dark_loss + bright_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'dark_fraction': dark_fraction.item(),\n",
    "            'bright_fraction': bright_fraction.item(),\n",
    "            'dark_loss': dark_loss.item(),\n",
    "            'bright_loss': bright_loss.item()\n",
    "        }\n",
    "\n",
    "# Test the loss function\n",
    "print(\"Bright/Dark Balance Loss implementation complete!\")\n",
    "print(\"\\nLoss function parameters:\")\n",
    "print(f\"  Dark threshold: 0.2\")\n",
    "print(f\"  Bright threshold: 0.9\")\n",
    "print(f\"  Dark target fraction: 0.05\")\n",
    "print(f\"  Bright target fraction: 0.01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bb4a9",
   "metadata": {},
   "source": [
    "## 2. Texture-Aware Lighting Maps\n",
    "\n",
    "This improvement uses gradient-respecting kernels to preserve texture while enhancing illumination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9696d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texture-Aware Smoothness Loss implementation complete!\n"
     ]
    }
   ],
   "source": [
    "class TextureAwareSmoothnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Gradient-aware illumination smoothness loss that respects texture boundaries.\n",
    "    This prevents harsh contrast jumps while maintaining smooth illumination maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TextureAwareSmoothnessLoss, self).__init__()\n",
    "        \n",
    "        # Sobel kernels for gradient computation\n",
    "        sobel_x = torch.FloatTensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.FloatTensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3)\n",
    "        \n",
    "        self.register_buffer('sobel_x', sobel_x)\n",
    "        self.register_buffer('sobel_y', sobel_y)\n",
    "        \n",
    "    def compute_gradient_magnitude(self, x):\n",
    "        \"\"\"Compute gradient magnitude map\"\"\"\n",
    "        # Convert to grayscale if RGB\n",
    "        if x.shape[1] == 3:\n",
    "            weights = torch.tensor([0.299, 0.587, 0.114], device=x.device).view(1, 3, 1, 1)\n",
    "            gray = torch.sum(x * weights, dim=1, keepdim=True)\n",
    "        else:\n",
    "            gray = x\n",
    "            \n",
    "        # Compute gradients\n",
    "        grad_x = F.conv2d(gray, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(gray, self.sobel_y, padding=1)\n",
    "        \n",
    "        # Gradient magnitude\n",
    "        grad_mag = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
    "        return grad_mag\n",
    "    \n",
    "    def forward(self, illumination_map, input_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            illumination_map: Curve parameter maps (B, 24, H, W) or similar\n",
    "            input_image: Original input image (B, 3, H, W) for gradient computation\n",
    "        Returns:\n",
    "            loss: Gradient-aware smoothness loss\n",
    "        \"\"\"\n",
    "        # Compute gradient magnitude of input image\n",
    "        input_grad = self.compute_gradient_magnitude(input_image)\n",
    "        \n",
    "        # Normalize gradient to [0, 1] for weighting\n",
    "        input_grad_norm = (input_grad - input_grad.min()) / (input_grad.max() - input_grad.min() + 1e-6)\n",
    "        \n",
    "        # Compute gradients of illumination map\n",
    "        # For each channel in illumination map\n",
    "        total_loss = 0.0\n",
    "        num_channels = illumination_map.shape[1]\n",
    "        \n",
    "        for i in range(num_channels):\n",
    "            channel_map = illumination_map[:, i:i+1, :, :]\n",
    "            \n",
    "            # Compute horizontal and vertical gradients\n",
    "            grad_x = torch.abs(channel_map[:, :, :, 1:] - channel_map[:, :, :, :-1])\n",
    "            grad_y = torch.abs(channel_map[:, :, 1:, :] - channel_map[:, :, :-1, :])\n",
    "            \n",
    "            # Apply gradient-aware weighting\n",
    "            # In high-gradient regions (texture boundaries), allow more variation\n",
    "            # In low-gradient regions (smooth areas), enforce more smoothness\n",
    "            weight_x = 1.0 / (input_grad_norm[:, :, :, :-1] + 0.1)  # Inverse weighting\n",
    "            weight_y = 1.0 / (input_grad_norm[:, :, :-1, :] + 0.1)\n",
    "            \n",
    "            # Weighted smoothness loss\n",
    "            channel_loss = torch.mean(weight_x * grad_x) + torch.mean(weight_y * grad_y)\n",
    "            total_loss += channel_loss\n",
    "        \n",
    "        return total_loss / num_channels\n",
    "\n",
    "print(\"Texture-Aware Smoothness Loss implementation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae2580",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccbbbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposure Fusion implementation complete!\n"
     ]
    }
   ],
   "source": [
    "class ExposureFusion:\n",
    "    \"\"\"\n",
    "    Multi-scale exposure fusion to combine Zero-DCE output with synthetic exposure brackets.\n",
    "    This helps preserve highlights while lifting shadows.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales=5, exposure_values=[-2, -1, 0, 1, 2]):\n",
    "        self.num_scales = num_scales\n",
    "        self.exposure_values = exposure_values  # Exposure values in EV\n",
    "        \n",
    "    def generate_exposure_brackets(self, image, base_exposure=0):\n",
    "        \"\"\"\n",
    "        Generate synthetic exposure brackets from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Tensor (B, C, H, W) in range [0, 1]\n",
    "            base_exposure: Base exposure value (0 = no change)\n",
    "        Returns:\n",
    "            brackets: List of exposure-bracketed images\n",
    "        \"\"\"\n",
    "        brackets = []\n",
    "        for ev in self.exposure_values:\n",
    "            # Exposure adjustment: multiply by 2^EV\n",
    "            exposure_factor = 2.0 ** ev\n",
    "            exposed = image * exposure_factor\n",
    "            # Clip to valid range\n",
    "            exposed = torch.clamp(exposed, 0.0, 1.0)\n",
    "            brackets.append(exposed)\n",
    "        return brackets\n",
    "    \n",
    "    def compute_weights(self, image):\n",
    "        \"\"\"\n",
    "        Compute fusion weights based on:\n",
    "        - Well-exposedness (closeness to 0.5)\n",
    "        - Saturation (colorfulness)\n",
    "        - Contrast (local variance)\n",
    "        \"\"\"\n",
    "        B, C, H, W = image.shape\n",
    "        \n",
    "        # Well-exposedness: Gaussian centered at 0.5\n",
    "        well_exposed = torch.exp(-0.5 * torch.pow(image - 0.5, 2) / (0.2 ** 2))\n",
    "        well_exposed = torch.mean(well_exposed, dim=1, keepdim=True)  # Average across channels\n",
    "        \n",
    "        # Saturation: standard deviation across color channels\n",
    "        saturation = torch.std(image, dim=1, keepdim=True)\n",
    "        \n",
    "        # Contrast: local variance using a simple kernel\n",
    "        kernel_size = 5\n",
    "        kernel = torch.ones(1, 1, kernel_size, kernel_size, device=image.device) / (kernel_size ** 2)\n",
    "        mean = F.conv2d(torch.mean(image, dim=1, keepdim=True), kernel, padding=kernel_size//2)\n",
    "        variance = F.conv2d(torch.pow(torch.mean(image, dim=1, keepdim=True) - mean, 2), \n",
    "                          kernel, padding=kernel_size//2)\n",
    "        contrast = variance\n",
    "        \n",
    "        # Normalize each component\n",
    "        well_exposed = (well_exposed - well_exposed.min()) / (well_exposed.max() - well_exposed.min() + 1e-6)\n",
    "        saturation = (saturation - saturation.min()) / (saturation.max() - saturation.min() + 1e-6)\n",
    "        contrast = (contrast - contrast.min()) / (contrast.max() - contrast.min() + 1e-6)\n",
    "        \n",
    "        # Combined weight\n",
    "        weight = well_exposed * saturation * (contrast + 0.1)\n",
    "        return weight\n",
    "    \n",
    "    def multi_scale_fusion(self, images, weights):\n",
    "        \"\"\"\n",
    "        Multi-scale fusion using Laplacian pyramid.\n",
    "        \n",
    "        Args:\n",
    "            images: List of images to fuse\n",
    "            weights: List of weight maps\n",
    "        Returns:\n",
    "            fused: Fused image\n",
    "        \"\"\"\n",
    "        # Simple weighted average (can be extended to Laplacian pyramid)\n",
    "        weights_sum = sum(weights)\n",
    "        weights_sum = torch.clamp(weights_sum, min=1e-6)\n",
    "        \n",
    "        fused = sum(img * w for img, w in zip(images, weights)) / weights_sum\n",
    "        return fused\n",
    "    \n",
    "    def fuse(self, zero_dce_output, input_image):\n",
    "        \"\"\"\n",
    "        Fuse Zero-DCE output with exposure brackets.\n",
    "        \n",
    "        Args:\n",
    "            zero_dce_output: Enhanced image from Zero-DCE (B, C, H, W)\n",
    "            input_image: Original input image (B, C, H, W)\n",
    "        Returns:\n",
    "            fused_image: Fused result\n",
    "        \"\"\"\n",
    "        # Generate exposure brackets from Zero-DCE output\n",
    "        brackets = self.generate_exposure_brackets(zero_dce_output)\n",
    "        \n",
    "        # Include Zero-DCE output as one of the brackets\n",
    "        all_images = [zero_dce_output] + brackets\n",
    "        \n",
    "        # Compute weights for each image\n",
    "        weights = [self.compute_weights(img) for img in all_images]\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        fused = self.multi_scale_fusion(all_images, weights)\n",
    "        \n",
    "        # Adaptive blending with Zero-DCE output based on local luminance\n",
    "        luma_weights = torch.tensor([0.299, 0.587, 0.114], device=zero_dce_output.device).view(1, 3, 1, 1)\n",
    "        luma = torch.sum(zero_dce_output * luma_weights, dim=1, keepdim=True)\n",
    "        \n",
    "        # In dark regions, use more of fused result; in bright regions, use more of Zero-DCE\n",
    "        blend_weight = torch.clamp(luma * 2.0, 0.0, 1.0)  # More fusion in dark areas\n",
    "        final = blend_weight * fused + (1 - blend_weight) * zero_dce_output\n",
    "        \n",
    "        return torch.clamp(final, 0.0, 1.0)\n",
    "\n",
    "print(\"Exposure Fusion implementation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ad810",
   "metadata": {},
   "source": [
    "## 4. Perceptual Co-training Loss\n",
    "\n",
    "This loss uses NIMA and NIQE metrics to optimize for perceptual quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28637bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptual Loss implementation complete!\n",
      "Note: Full NIMA/NIQE integration requires their pre-trained models.\n"
     ]
    }
   ],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG features (similar to existing perception_loss in Myloss.py)\n",
    "    Extended to work with NIMA/NIQE concepts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        # Use VGG features for perceptual similarity\n",
    "        # This is a simplified version - full NIMA/NIQE integration would require their models\n",
    "        try:\n",
    "            from torchvision.models import vgg16\n",
    "            vgg = vgg16(pretrained=True).features\n",
    "            self.feature_extractor = nn.Sequential(*list(vgg.children())[:23])  # Up to relu4_3\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        except:\n",
    "            print(\"Warning: Could not load VGG16. Using simplified perceptual loss.\")\n",
    "            self.feature_extractor = None\n",
    "    \n",
    "    def forward(self, enhanced_image, target_image=None):\n",
    "        \"\"\"\n",
    "        Compute perceptual loss.\n",
    "        If target_image is None, assumes we want to maximize quality (no reference).\n",
    "        \"\"\"\n",
    "        if self.feature_extractor is None:\n",
    "            # Fallback: use simple L2 loss on image\n",
    "            return torch.mean(torch.pow(enhanced_image, 2))\n",
    "        \n",
    "        # Extract features\n",
    "        enhanced_features = self.feature_extractor(enhanced_image)\n",
    "        \n",
    "        if target_image is not None:\n",
    "            # Reference-based perceptual loss\n",
    "            target_features = self.feature_extractor(target_image)\n",
    "            loss = F.mse_loss(enhanced_features, target_features)\n",
    "        else:\n",
    "            # No-reference: encourage natural-looking features\n",
    "            # Penalize extreme values (overexposure/underexposure)\n",
    "            loss = torch.mean(torch.pow(enhanced_features - 0.5, 2))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Note: Full NIMA/NIQE integration would require:\n",
    "# 1. Loading pre-trained NIMA model for aesthetic scoring\n",
    "# 2. Computing NIQE score (typically non-differentiable, may need approximation)\n",
    "# For now, we use VGG-based perceptual loss as a proxy\n",
    "\n",
    "print(\"Perceptual Loss implementation complete!\")\n",
    "print(\"Note: Full NIMA/NIQE integration requires their pre-trained models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7e472",
   "metadata": {},
   "source": [
    "## 10. Start Training with Improved Model\n",
    "\n",
    "This cell starts training the model with all improvements. You can configure which improvements to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Training data not found, using test data from: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/data/test_data/DICM\n",
      "Training Configuration:\n",
      "  Data path: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/data/test_data/DICM\n",
      "  Snapshots folder: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/snapshots/improved\n",
      "  Learning rate: 0.0001\n",
      "  Batch size: 8\n",
      "  Number of epochs: 50\n",
      "  Load pretrain: True\n",
      "  Pretrain path: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/snapshots/Epoch99.pth\n",
      "\n",
      "Improvements enabled:\n",
      "  Bright/Dark Balance: True\n",
      "  Texture-Aware: True\n",
      "  Perceptual: True\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training\"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        # Try train_data first, fallback to test_data if not available\n",
    "        train_data_path = PROJECT_ROOT / \"data/train_data\"\n",
    "        test_data_path = PROJECT_ROOT / \"data/test_data/DICM\"\n",
    "        \n",
    "        if train_data_path.exists() and len(list(train_data_path.glob(\"*.jpg\"))) > 0:\n",
    "            # Ensure path ends with / for glob to work correctly\n",
    "            self.lowlight_images_path = str(train_data_path) + \"/\"\n",
    "        elif test_data_path.exists() and len(list(test_data_path.glob(\"*.jpg\"))) > 0:\n",
    "            print(f\"⚠ Training data not found, using test data from: {test_data_path}\")\n",
    "            # Ensure path ends with / for glob to work correctly\n",
    "            self.lowlight_images_path = str(test_data_path) + \"/\"\n",
    "        else:\n",
    "            # Will show error later, but ensure path format is correct\n",
    "            self.lowlight_images_path = str(train_data_path) + \"/\"\n",
    "        \n",
    "        self.snapshots_folder = str(PROJECT_ROOT / \"snapshots/improved/\")\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.lr = 0.0001\n",
    "        self.weight_decay = 0.0001\n",
    "        self.grad_clip_norm = 0.1\n",
    "        self.num_epochs = 50  # Start with fewer epochs for testing\n",
    "        self.train_batch_size = 8\n",
    "        self.num_workers = 4 if device.type == 'cuda' else 0\n",
    "        \n",
    "        # Logging and checkpointing\n",
    "        self.display_iter = 10\n",
    "        self.snapshot_iter = 100  # Save checkpoint every N iterations\n",
    "        \n",
    "        # Pre-trained model\n",
    "        self.load_pretrain = True\n",
    "        self.pretrain_dir = str(PROJECT_ROOT / \"snapshots/Epoch99.pth\")\n",
    "        \n",
    "        # Improvement flags (start with one at a time for testing)\n",
    "        self.use_bright_dark = True\n",
    "        self.use_texture_aware = True\n",
    "        self.use_perceptual = True\n",
    "        \n",
    "        # Loss weights (can be adjusted)\n",
    "        self.loss_weights = {\n",
    "            'tv': 200.0,\n",
    "            'spa': 1.0,\n",
    "            'color': 5.0,\n",
    "            'exp': 10.0,\n",
    "            'bright_dark': 2.0,  # Adjust based on results\n",
    "            'texture_aware': 1.0,\n",
    "            'perceptual': 0.5\n",
    "        }\n",
    "\n",
    "# Create config\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Create snapshots directory\n",
    "os.makedirs(config.snapshots_folder, exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Data path: {config.lowlight_images_path}\")\n",
    "print(f\"  Snapshots folder: {config.snapshots_folder}\")\n",
    "print(f\"  Learning rate: {config.lr}\")\n",
    "print(f\"  Batch size: {config.train_batch_size}\")\n",
    "print(f\"  Number of epochs: {config.num_epochs}\")\n",
    "print(f\"  Load pretrain: {config.load_pretrain}\")\n",
    "if config.load_pretrain:\n",
    "    print(f\"  Pretrain path: {config.pretrain_dir}\")\n",
    "print(f\"\\nImprovements enabled:\")\n",
    "print(f\"  Bright/Dark Balance: {config.use_bright_dark}\")\n",
    "print(f\"  Texture-Aware: {config.use_texture_aware}\")\n",
    "print(f\"  Perceptual: {config.use_perceptual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51008252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Loading pre-trained weights from: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/snapshots/Epoch99.pth\n",
      "✓ Pre-trained weights loaded successfully\n",
      "\n",
      "Loading training data from: /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/data/test_data/DICM\n",
      "✓ Found 64 image files in directory\n",
      "Total training examples: 0\n",
      "✗ Error loading dataset: Dataset is empty after loading. Check dataloader implementation.\n",
      "\n",
      "Troubleshooting:\n",
      "1. Check that /Users/mdshahzeb/Documents/GitHub/Zero-DCE-improved/Zero-DCE_code/data/test_data/DICM contains image files (.jpg, .png, .bmp)\n",
      "2. Check dataloader.py - it expects *.jpg files in the directory\n",
      "3. If using test_data, images should be directly in the folder (not in subfolders)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dataset is empty after loading. Check dataloader implementation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mlowlight_loader(config\u001b[38;5;241m.\u001b[39mlowlight_images_path)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset is empty after loading. Check dataloader implementation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     72\u001b[0m     train_dataset, \n\u001b[1;32m     73\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m(device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Training dataset loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset is empty after loading. Check dataloader implementation."
     ]
    }
   ],
   "source": [
    "# Initialize Model and Training Components\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize model weights\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "DCE_net = model.enhance_net_nopool().to(device)\n",
    "DCE_net.apply(weights_init)\n",
    "\n",
    "# Load pre-trained weights if specified\n",
    "if config.load_pretrain and os.path.exists(config.pretrain_dir):\n",
    "    print(f\"Loading pre-trained weights from: {config.pretrain_dir}\")\n",
    "    try:\n",
    "        state_dict = torch.load(config.pretrain_dir, map_location=device)\n",
    "        DCE_net.load_state_dict(state_dict)\n",
    "        print(\"✓ Pre-trained weights loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Could not load pre-trained weights: {e}\")\n",
    "        print(\"Starting from scratch...\")\n",
    "else:\n",
    "    print(\"Starting training from scratch (no pre-trained weights)\")\n",
    "\n",
    "# Initialize data loader\n",
    "print(f\"\\nLoading training data from: {config.lowlight_images_path}\")\n",
    "\n",
    "# Check if training data directory exists\n",
    "train_data_path = Path(config.lowlight_images_path)\n",
    "if not train_data_path.exists():\n",
    "    print(f\"✗ Training data directory not found: {train_data_path}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DATA NOT FOUND\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTo download training data:\")\n",
    "    print(\"1. Google Drive: https://drive.google.com/file/d/1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3/view?usp=sharing\")\n",
    "    print(\"2. Baidu Cloud: https://pan.baidu.com/s/11-u_FZkJ8OgbqcG6763XyA (password: 1234)\")\n",
    "    print(\"3. Extract and place in: data/train_data/\")\n",
    "    print(\"\\nTrying to use test_data as fallback...\")\n",
    "    \n",
    "    # Try test_data as fallback\n",
    "    test_data_path = PROJECT_ROOT / \"data/test_data/DICM\"\n",
    "    if test_data_path.exists():\n",
    "        # Ensure path ends with / for glob to work correctly\n",
    "        config.lowlight_images_path = str(test_data_path) + \"/\"\n",
    "        train_data_path = test_data_path\n",
    "        print(f\"✓ Using test data from: {test_data_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Training data not found at {train_data_path}. Please download training data.\")\n",
    "\n",
    "# Check if directory has images\n",
    "image_files = list(train_data_path.glob(\"*.jpg\")) + list(train_data_path.glob(\"*.png\")) + list(train_data_path.glob(\"*.bmp\"))\n",
    "if len(image_files) == 0:\n",
    "    print(f\"✗ No image files found in: {train_data_path}\")\n",
    "    print(f\"  Supported formats: .jpg, .png, .bmp\")\n",
    "    print(f\"  Found files: {list(train_data_path.glob('*'))[:10]}\")\n",
    "    raise ValueError(f\"No training images found in {train_data_path}\")\n",
    "\n",
    "print(f\"✓ Found {len(image_files)} image files in directory\")\n",
    "\n",
    "# Ensure path ends with / for glob pattern to work\n",
    "if not config.lowlight_images_path.endswith(\"/\"):\n",
    "    config.lowlight_images_path = config.lowlight_images_path + \"/\"\n",
    "    print(f\"  Adjusted path to: {config.lowlight_images_path}\")\n",
    "\n",
    "# Test glob pattern before creating dataset\n",
    "import glob\n",
    "test_glob = glob.glob(config.lowlight_images_path + \"*.jpg\")\n",
    "print(f\"  Test glob found {len(test_glob)} .jpg files\")\n",
    "\n",
    "if len(test_glob) == 0:\n",
    "    # Try other formats\n",
    "    test_glob_png = glob.glob(config.lowlight_images_path + \"*.png\")\n",
    "    test_glob_bmp = glob.glob(config.lowlight_images_path + \"*.bmp\")\n",
    "    print(f\"  Found {len(test_glob_png)} .png files, {len(test_glob_bmp)} .bmp files\")\n",
    "    if len(test_glob_png) == 0 and len(test_glob_bmp) == 0:\n",
    "        raise ValueError(f\"No image files found with glob pattern: {config.lowlight_images_path}*.jpg\")\n",
    "\n",
    "try:\n",
    "    train_dataset = dataloader.lowlight_loader(config.lowlight_images_path)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        raise ValueError(f\"Dataset is empty after loading. Glob pattern: {config.lowlight_images_path}*.jpg\")\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.train_batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=(device.type == 'cuda')\n",
    "    )\n",
    "    print(f\"✓ Training dataset loaded: {len(train_dataset)} images\")\n",
    "    print(f\"  Batch size: {config.train_batch_size}\")\n",
    "    print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading dataset: {e}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"1. Path used: {config.lowlight_images_path}\")\n",
    "    print(f\"2. Glob pattern: {config.lowlight_images_path}*.jpg\")\n",
    "    print(f\"3. Check that the directory contains .jpg files\")\n",
    "    print(f\"4. The path must end with '/' for glob to work correctly\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Create improved loss function\n",
    "print(\"\\nCreating improved loss function...\")\n",
    "compute_loss = create_improved_loss_function(\n",
    "    use_bright_dark=config.use_bright_dark,\n",
    "    use_texture_aware=config.use_texture_aware,\n",
    "    use_perceptual=config.use_perceptual,\n",
    "    device=device\n",
    ")\n",
    "print(\"✓ Loss function created\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(\n",
    "    DCE_net.parameters(), \n",
    "    lr=config.lr, \n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "print(f\"✓ Optimizer initialized (Adam, lr={config.lr})\")\n",
    "\n",
    "# Set model to training mode\n",
    "DCE_net.train()\n",
    "print(\"\\n✓ Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de129c",
   "metadata": {},
   "source": [
    "## 11. Training Loop\n",
    "\n",
    "Run this cell to start training. The training will:\n",
    "- Use all enabled improvements\n",
    "- Save checkpoints periodically\n",
    "- Display loss information\n",
    "- Track dark/bright pixel fractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Training with Improved Zero-DCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total epochs: {config.num_epochs}\")\n",
    "print(f\"Total iterations per epoch: {len(train_loader)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training statistics\n",
    "total_iterations = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for iteration, img_lowlight in enumerate(train_loader):\n",
    "            total_iterations += 1\n",
    "            \n",
    "            # Move data to device\n",
    "            img_lowlight = img_lowlight.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            enhanced_image_1, enhanced_image, A = DCE_net(img_lowlight)\n",
    "            \n",
    "            # Compute loss with improvements\n",
    "            loss, loss_dict = compute_loss(\n",
    "                enhanced_image_1, \n",
    "                enhanced_image, \n",
    "                A, \n",
    "                img_lowlight,\n",
    "                loss_weights=config.loss_weights\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(DCE_net.parameters(), config.grad_clip_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            epoch_losses.append(loss_dict['total'])\n",
    "            \n",
    "            # Display progress\n",
    "            if (iteration + 1) % config.display_iter == 0:\n",
    "                elapsed = time.time() - epoch_start_time\n",
    "                avg_loss = np.mean(epoch_losses[-config.display_iter:])\n",
    "                \n",
    "                print(f\"\\nEpoch [{epoch+1}/{config.num_epochs}], Iteration [{iteration+1}/{len(train_loader)}]\")\n",
    "                print(f\"  Time: {elapsed:.2f}s | Avg Loss: {avg_loss:.4f}\")\n",
    "                print(f\"  Loss breakdown:\")\n",
    "                print(f\"    TV: {loss_dict.get('tv', 0):.4f} | \"\n",
    "                      f\"Spatial: {loss_dict.get('spa', 0):.4f} | \"\n",
    "                      f\"Color: {loss_dict.get('color', 0):.4f} | \"\n",
    "                      f\"Exposure: {loss_dict.get('exp', 0):.4f}\")\n",
    "                \n",
    "                if config.use_bright_dark and 'bd_dark_fraction' in loss_dict:\n",
    "                    print(f\"    Dark Fraction: {loss_dict.get('bd_dark_fraction', 0):.4f} | \"\n",
    "                          f\"Bright Fraction: {loss_dict.get('bd_bright_fraction', 0):.6f}\")\n",
    "                \n",
    "                if config.use_texture_aware:\n",
    "                    print(f\"    Texture-Aware: {loss_dict.get('texture_aware', 0):.4f}\")\n",
    "                \n",
    "                if config.use_perceptual:\n",
    "                    print(f\"    Perceptual: {loss_dict.get('perceptual', 0):.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (iteration + 1) % config.snapshot_iter == 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    config.snapshots_folder, \n",
    "                    f\"Epoch{epoch+1}_Iter{iteration+1}_improved.pth\"\n",
    "                )\n",
    "                torch.save(DCE_net.state_dict(), checkpoint_path)\n",
    "                print(f\"  ✓ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # End of epoch\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch [{epoch+1}/{config.num_epochs}] completed\")\n",
    "        print(f\"  Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s ({epoch_time/60:.2f} minutes)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint = os.path.join(\n",
    "            config.snapshots_folder,\n",
    "            f\"Epoch{epoch+1}_improved.pth\"\n",
    "        )\n",
    "        torch.save(DCE_net.state_dict(), epoch_checkpoint)\n",
    "        print(f\"✓ Epoch checkpoint saved: {epoch_checkpoint}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            best_checkpoint = os.path.join(config.snapshots_folder, \"best_improved.pth\")\n",
    "            torch.save(DCE_net.state_dict(), best_checkpoint)\n",
    "            print(f\"✓ New best model saved: {best_checkpoint} (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user!\")\n",
    "    print(\"Saving current model state...\")\n",
    "    interrupt_checkpoint = os.path.join(\n",
    "        config.snapshots_folder,\n",
    "        f\"interrupted_Epoch{epoch+1}_Iter{iteration+1}.pth\"\n",
    "    )\n",
    "    torch.save(DCE_net.state_dict(), interrupt_checkpoint)\n",
    "    print(f\"✓ Model saved: {interrupt_checkpoint}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n✗ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nSaving current model state...\")\n",
    "    error_checkpoint = os.path.join(\n",
    "        config.snapshots_folder,\n",
    "        f\"error_Epoch{epoch+1}_Iter{iteration+1}.pth\"\n",
    "    )\n",
    "    torch.save(DCE_net.state_dict(), error_checkpoint)\n",
    "    print(f\"✓ Model saved: {error_checkpoint}\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total iterations: {total_iterations}\")\n",
    "    print(f\"Checkpoints saved in: {config.snapshots_folder}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc088a",
   "metadata": {},
   "source": [
    "## 5. Integrated Training Function\n",
    "\n",
    "Combine all improvements into a unified training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f15f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated loss function created!\n"
     ]
    }
   ],
   "source": [
    "def create_improved_loss_function(use_bright_dark=True, use_texture_aware=True, \n",
    "                                   use_perceptual=True, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a combined loss function with all improvements.\n",
    "    \n",
    "    Returns:\n",
    "        loss_fn: Function that computes total loss\n",
    "        loss_components: Dictionary to store individual loss values\n",
    "    \"\"\"\n",
    "    # Baseline losses\n",
    "    L_color = Myloss.L_color().to(device)\n",
    "    L_spa = Myloss.L_spa().to(device)\n",
    "    L_exp = Myloss.L_exp(16, 0.6).to(device)\n",
    "    L_TV = Myloss.L_TV().to(device)\n",
    "    \n",
    "    # New losses\n",
    "    bright_dark_loss = BrightDarkBalanceLoss().to(device) if use_bright_dark else None\n",
    "    texture_aware_loss = TextureAwareSmoothnessLoss().to(device) if use_texture_aware else None\n",
    "    perceptual_loss = PerceptualLoss().to(device) if use_perceptual else None\n",
    "    \n",
    "    def compute_loss(enhanced_image_1, enhanced_image, A, img_lowlight, \n",
    "                     loss_weights=None):\n",
    "        \"\"\"\n",
    "        Compute combined loss with all improvements.\n",
    "        \n",
    "        Args:\n",
    "            enhanced_image_1: Intermediate enhanced image\n",
    "            enhanced_image: Final enhanced image\n",
    "            A: Illumination adjustment curves (24 channels)\n",
    "            img_lowlight: Original low-light input\n",
    "            loss_weights: Dictionary of loss weights (optional)\n",
    "        Returns:\n",
    "            total_loss: Combined loss\n",
    "            loss_dict: Dictionary of individual losses\n",
    "        \"\"\"\n",
    "        if loss_weights is None:\n",
    "            loss_weights = {\n",
    "                'tv': 200.0,\n",
    "                'spa': 1.0,\n",
    "                'color': 5.0,\n",
    "                'exp': 10.0,\n",
    "                'bright_dark': 2.0,\n",
    "                'texture_aware': 1.0,\n",
    "                'perceptual': 0.5\n",
    "            }\n",
    "        \n",
    "        loss_dict = {}\n",
    "        \n",
    "        # Baseline losses\n",
    "        loss_tv = loss_weights['tv'] * L_TV(A)\n",
    "        loss_spa = loss_weights['spa'] * torch.mean(L_spa(enhanced_image, img_lowlight))\n",
    "        loss_col = loss_weights['color'] * torch.mean(L_color(enhanced_image))\n",
    "        loss_exp = loss_weights['exp'] * torch.mean(L_exp(enhanced_image))\n",
    "        \n",
    "        loss_dict['tv'] = loss_tv.item()\n",
    "        loss_dict['spa'] = loss_spa.item()\n",
    "        loss_dict['color'] = loss_col.item()\n",
    "        loss_dict['exp'] = loss_exp.item()\n",
    "        \n",
    "        total_loss = loss_tv + loss_spa + loss_col + loss_exp\n",
    "        \n",
    "        # New losses\n",
    "        if bright_dark_loss is not None:\n",
    "            bd_loss, bd_info = bright_dark_loss(enhanced_image)\n",
    "            total_loss += loss_weights['bright_dark'] * bd_loss\n",
    "            loss_dict['bright_dark'] = bd_loss.item()\n",
    "            loss_dict.update({f'bd_{k}': v for k, v in bd_info.items()})\n",
    "        \n",
    "        if texture_aware_loss is not None:\n",
    "            ta_loss = texture_aware_loss(A, img_lowlight)\n",
    "            total_loss += loss_weights['texture_aware'] * ta_loss\n",
    "            loss_dict['texture_aware'] = ta_loss.item()\n",
    "        \n",
    "        if perceptual_loss is not None:\n",
    "            perc_loss = perceptual_loss(enhanced_image)\n",
    "            total_loss += loss_weights['perceptual'] * perc_loss\n",
    "            loss_dict['perceptual'] = perc_loss.item()\n",
    "        \n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "    \n",
    "    return compute_loss\n",
    "\n",
    "print(\"Integrated loss function created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049c798",
   "metadata": {},
   "source": [
    "## 6. Testing and Evaluation\n",
    "\n",
    "Let's test the improvements on a sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e445253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing functions ready!\n",
      "\n",
      "To test, run:\n",
      "results = test_improvements('path/to/image.jpg', 'snapshots/Epoch99.pth', device)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "def load_model(weights_path, device):\n",
    "    \"\"\"Load Zero-DCE model\"\"\"\n",
    "    net = model.enhance_net_nopool().to(device)\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "# Test on a sample image\n",
    "def test_improvements(image_path, model_path, device='cuda', use_fusion=True):\n",
    "    \"\"\"\n",
    "    Test all improvements on a single image.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    net = load_model(model_path, device)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.asarray(img).astype(np.float32) / 255.0\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Baseline enhancement\n",
    "    with torch.no_grad():\n",
    "        enhanced_1, enhanced, curves = net(img_tensor)\n",
    "    \n",
    "    # Apply exposure fusion if enabled\n",
    "    if use_fusion:\n",
    "        fusion = ExposureFusion()\n",
    "        enhanced_fused = fusion.fuse(enhanced, img_tensor)\n",
    "    else:\n",
    "        enhanced_fused = enhanced\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    def tensor_to_numpy(t):\n",
    "        t = t.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "        return np.clip(t, 0, 1)\n",
    "    \n",
    "    original = tensor_to_numpy(img_tensor)\n",
    "    enhanced_np = tensor_to_numpy(enhanced)\n",
    "    enhanced_fused_np = tensor_to_numpy(enhanced_fused) if use_fusion else None\n",
    "    \n",
    "    return {\n",
    "        'original': original,\n",
    "        'enhanced': enhanced_np,\n",
    "        'enhanced_fused': enhanced_fused_np,\n",
    "        'curves': curves\n",
    "    }\n",
    "\n",
    "print(\"Testing functions ready!\")\n",
    "print(\"\\nTo test, run:\")\n",
    "print(\"results = test_improvements('path/to/image.jpg', 'snapshots/Epoch99.pth', device)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e6640",
   "metadata": {},
   "source": [
    "## 7. Visualization Helper\n",
    "\n",
    "Function to visualize results and compare baseline vs improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d9c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization and metrics functions ready!\n"
     ]
    }
   ],
   "source": [
    "def visualize_results(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize original, enhanced, and fused results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3 if results['enhanced_fused'] is not None else 2, \n",
    "                            figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(results['original'])\n",
    "    axes[0].set_title('Original Low-Light Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(results['enhanced'])\n",
    "    axes[1].set_title('Zero-DCE Enhanced')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    if results['enhanced_fused'] is not None:\n",
    "        axes[2].imshow(results['enhanced_fused'])\n",
    "        axes[2].set_title('With Exposure Fusion')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def compute_metrics(image):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for an image.\n",
    "    \"\"\"\n",
    "    # Convert to luminance\n",
    "    if len(image.shape) == 3:\n",
    "        luma = 0.299 * image[:, :, 0] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 2]\n",
    "    else:\n",
    "        luma = image\n",
    "    \n",
    "    # Dark pixel fraction\n",
    "    dark_fraction = np.mean(luma < 0.2)\n",
    "    \n",
    "    # Bright pixel fraction\n",
    "    bright_fraction = np.mean(luma > 0.9)\n",
    "    \n",
    "    # Patch contrast (4x4 grid)\n",
    "    h, w = luma.shape\n",
    "    patch_h, patch_w = h // 4, w // 4\n",
    "    patches = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            patch = luma[i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w]\n",
    "            patches.append(np.std(patch))\n",
    "    patch_contrast = np.mean(patches)\n",
    "    \n",
    "    return {\n",
    "        'dark_fraction': dark_fraction,\n",
    "        'bright_fraction': bright_fraction,\n",
    "        'patch_contrast': patch_contrast,\n",
    "        'mean_luminance': np.mean(luma)\n",
    "    }\n",
    "\n",
    "print(\"Visualization and metrics functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d3cc6",
   "metadata": {},
   "source": [
    "## 8. Example Usage\n",
    "\n",
    "Test the improvements on a sample image from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57346410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to test! Uncomment the code above and adjust paths as needed.\n"
     ]
    }
   ],
   "source": [
    "# Example: Test on a sample image\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# # Set paths\n",
    "# test_image_path = PROJECT_ROOT / \"data/test_data/DICM/01.jpg\"\n",
    "# model_path = PROJECT_ROOT / \"snapshots/Epoch99.pth\"\n",
    "# \n",
    "# if test_image_path.exists() and model_path.exists():\n",
    "#     print(\"Testing improvements on sample image...\")\n",
    "#     results = test_improvements(str(test_image_path), str(model_path), device)\n",
    "#     \n",
    "#     # Compute metrics\n",
    "#     orig_metrics = compute_metrics(results['original'])\n",
    "#     enh_metrics = compute_metrics(results['enhanced'])\n",
    "#     \n",
    "#     print(\"\\n=== Metrics Comparison ===\")\n",
    "#     print(f\"Dark Pixel Fraction: {orig_metrics['dark_fraction']:.4f} -> {enh_metrics['dark_fraction']:.4f}\")\n",
    "#     print(f\"Bright Pixel Fraction: {orig_metrics['bright_fraction']:.6f} -> {enh_metrics['bright_fraction']:.6f}\")\n",
    "#     print(f\"Patch Contrast: {orig_metrics['patch_contrast']:.4f} -> {enh_metrics['patch_contrast']:.4f}\")\n",
    "#     \n",
    "#     # Visualize\n",
    "#     visualize_results(results)\n",
    "# else:\n",
    "#     print(\"Test image or model not found. Please check paths.\")\n",
    "\n",
    "print(\"Ready to test! Uncomment the code above and adjust paths as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fd7b2",
   "metadata": {},
   "source": [
    "## 9. Training Script Integration\n",
    "\n",
    "To use these improvements in training, integrate them into the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87e8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function with improvements ready!\n",
      "\n",
      "To use, create a config object and call:\n",
      "  model = train_with_improvements(config)\n"
     ]
    }
   ],
   "source": [
    "# Example training loop with improvements\n",
    "# This can be integrated into lowlight_train.py\n",
    "\n",
    "def train_with_improvements(config, use_bright_dark=True, use_texture_aware=True, \n",
    "                            use_perceptual=True, use_fusion=False):\n",
    "    \"\"\"\n",
    "    Training function with all improvements integrated.\n",
    "    Note: use_fusion=False during training (only for inference)\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    DCE_net = model.enhance_net_nopool().to(device)\n",
    "    DCE_net.apply(lambda m: m.weight.data.normal_(0.0, 0.02) if isinstance(m, nn.Conv2d) else None)\n",
    "    \n",
    "    if config.load_pretrain:\n",
    "        DCE_net.load_state_dict(torch.load(config.pretrain_dir, map_location=device))\n",
    "    \n",
    "    # Data loader\n",
    "    train_dataset = dataloader.lowlight_loader(config.lowlight_images_path)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.train_batch_size, \n",
    "        shuffle=True, num_workers=config.num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Loss function with improvements\n",
    "    compute_loss = create_improved_loss_function(\n",
    "        use_bright_dark=use_bright_dark,\n",
    "        use_texture_aware=use_texture_aware,\n",
    "        use_perceptual=use_perceptual,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(DCE_net.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    DCE_net.train()\n",
    "    \n",
    "    print(\"Starting training with improvements...\")\n",
    "    print(f\"  Bright/Dark Balance: {use_bright_dark}\")\n",
    "    print(f\"  Texture-Aware: {use_texture_aware}\")\n",
    "    print(f\"  Perceptual: {use_perceptual}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        for iteration, img_lowlight in enumerate(train_loader):\n",
    "            img_lowlight = img_lowlight.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            enhanced_image_1, enhanced_image, A = DCE_net(img_lowlight)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_dict = compute_loss(enhanced_image_1, enhanced_image, A, img_lowlight)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(DCE_net.parameters(), config.grad_clip_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logging\n",
    "            if (iteration + 1) % config.display_iter == 0:\n",
    "                print(f\"Epoch {epoch}, Iteration {iteration+1}\")\n",
    "                print(f\"  Total Loss: {loss_dict['total']:.4f}\")\n",
    "                if 'bright_dark' in loss_dict:\n",
    "                    print(f\"  Dark Fraction: {loss_dict.get('bd_dark_fraction', 0):.4f}, \"\n",
    "                          f\"Bright Fraction: {loss_dict.get('bd_bright_fraction', 0):.6f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (iteration + 1) % config.snapshot_iter == 0:\n",
    "                save_path = config.snapshots_folder + f\"Epoch{epoch}_improved.pth\"\n",
    "                torch.save(DCE_net.state_dict(), save_path)\n",
    "                print(f\"  Saved checkpoint: {save_path}\")\n",
    "    \n",
    "    return DCE_net\n",
    "\n",
    "print(\"Training function with improvements ready!\")\n",
    "print(\"\\nTo use, create a config object and call:\")\n",
    "print(\"  model = train_with_improvements(config)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4974a9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements all four improvements from the survey:\n",
    "\n",
    "1. ✅ **Bright/Dark Balance Loss** - Dual-histogram regularization\n",
    "2. ✅ **Texture-Aware Lighting Maps** - Gradient-respecting smoothness\n",
    "3. ✅ **Hybrid Exposure Fusion** - Multi-exposure fusion (for inference)\n",
    "4. ✅ **Perceptual Co-training** - VGG-based perceptual loss\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Test individual components** on sample images\n",
    "2. **Fine-tune loss weights** for optimal balance\n",
    "3. **Train model** with improvements (start with one improvement at a time)\n",
    "4. **Evaluate** on test datasets and compare with baseline\n",
    "5. **Iterate** based on results\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- Start by testing with `use_bright_dark=True` only, then add others incrementally\n",
    "- Adjust loss weights in `create_improved_loss_function()` based on results\n",
    "- Use exposure fusion only during inference (not training) to save computation\n",
    "- Monitor dark/bright pixel fractions during training to track improvement\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
